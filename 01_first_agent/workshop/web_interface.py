# web_interface.py
import streamlit as st
import asyncio
import logging
from strands import Agent
from strands.models import BedrockModel
from strands_tools import current_time, http_request
from strands.models.ollama import OllamaModel

logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s | %(name)s | %(message)s",
    handlers=[logging.StreamHandler()]
)
logging.getLogger("strands").setLevel(logging.DEBUG)

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Strands Agents å°åŠ©æ‰‹",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .tool-call-box {
        background-color: #f0f2f6;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 10px;
    }
    .tool-name {
        font-weight: bold;
        color: #0068c9;
    }
    .tool-params {
        font-family: monospace;
        background-color: #f8f9fa;
        padding: 5px;
        border-radius: 3px;
    }
</style>
""", unsafe_allow_html=True)

# å¯ç”¨çš„æ¨¡å‹é…ç½®


# åˆå§‹åŒ–Agent
def init_agent(model_id="qwen3:1.7b"):
    try:
        model = OllamaModel(
            host="http://127.0.0.1:11434",
            model_id=model_id
        )
        return Agent(
            system_prompt="""
            ä½ æ˜¯ä¸€ä¸ªä¸­å›½å›½å†…çš„ç”Ÿæ´»åŠ©æ‰‹ï¼Œè¿ç”¨ç§‘å­¦çš„çŸ¥è¯†å›ç­”å„ç§é—®é¢˜ã€‚
            å¦‚æœç”¨æˆ·é—®é—®é¢˜ï¼Œè¯·å°½é‡è°ƒç”¨ http_request å·¥å…·æŸ¥è¯¢ä¸­å›½å›½å†…çš„ç™¾ç§‘ç½‘ç«™ã€‚
            """,
            model=model,
            tools=[current_time, http_request],
            callback_handler=None
        )
    except Exception as e:
        # è¿™é‡ŒæŠ›ç»™ä¸Šå±‚ï¼Œç”±ç•Œé¢å±•ç¤ºå‹å¥½æç¤º
        raise RuntimeError(
            f"åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹å¤±è´¥ï¼š{e}\n"
            "è¯·ç¡®è®¤å·²å®‰è£…å¹¶å¯åŠ¨ Ollamaï¼ˆ`ollama serve`ï¼‰ï¼Œä¸”å·² pull å¯¹åº”æ¨¡å‹ï¼ˆå¦‚ `ollama pull qwen3:1.7b`ï¼‰ã€‚"
        )


# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []

if "selected_model" not in st.session_state:
    st.session_state.selected_model = "qwen3:1.7b"  # é»˜è®¤æ¨¡å‹

if "agent" not in st.session_state:
    st.session_state.agent = init_agent(st.session_state.selected_model)

async def process_user_input_streaming(prompt):
    """ä½¿ç”¨å¼‚æ­¥æµå¼å¤„ç†ç”¨æˆ·è¾“å…¥"""
    try:
        # ä½¿ç”¨å¼‚æ­¥æµå¼è¾“å‡º
        full_response = ""
        async for event in st.session_state.agent.stream_async(prompt):
            if "data" in event:
                # ç´¯ç§¯æ–‡æœ¬
                full_response += event["data"]
        
        # è¿”å›å®Œæ•´å“åº”
        return full_response
        
    except Exception as e:
        return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}"

def process_user_input(prompt):
    """å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆç”¨äºç¤ºä¾‹æŒ‰é’®ï¼‰"""
    try:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        logging.info("prompt æ˜¯ï¼š" +prompt) 
        response = st.session_state.agent(prompt)

        # æ·»åŠ AIå›å¤åˆ°å†å²
        st.session_state.messages.append({"role": "assistant", "content": response})
        
        return True
        
    except Exception as e:
        error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}"
        st.session_state.messages.append({"role": "assistant", "content": error_msg})
        return False

def main():
    st.title("Strands AIåŠ©æ‰‹")
    st.markdown("---")
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("åŠŸèƒ½ä»‹ç»")
        st.markdown("""
        **AIåŠ©æ‰‹åŠŸèƒ½ï¼š**
        - è·å–å½“å‰æ—¶é—´
        - ç½‘ç»œä¿¡æ¯æœç´¢
        - çŸ¥è¯†é—®ç­”
        - ç”Ÿæ´»å»ºè®®
        
        **ä½¿ç”¨æ–¹æ³•ï¼š**
        åœ¨ä¸‹æ–¹è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ŒAIåŠ©æ‰‹å°†ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚
        """)
        
        # ç¤ºä¾‹é—®é¢˜æ”¾åœ¨ä¾§è¾¹æ ï¼ˆåªå±•ç¤ºï¼Œä¸èƒ½ç‚¹å‡»ï¼‰
        st.markdown("---")
        st.markdown("**è¯•è¯•è¿™äº›é—®é¢˜ï¼š**")
        
        # ä½¿ç”¨æ™®é€šæ–‡æœ¬æ˜¾ç¤ºç¤ºä¾‹é—®é¢˜ï¼Œç¡®ä¿æ¯ä¸ªé—®é¢˜å•ç‹¬ä¸€è¡Œ
        st.markdown("â€¢ ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ")
        st.markdown("â€¢ å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æƒ…å†µ")
        st.markdown("â€¢ ä»€ä¹ˆæ˜¯æ¢…é›¨ï¼Ÿè¯·è¯¦ç»†è§£é‡Šä¸€ä¸‹")
        
        # æ·»åŠ æç¤º
        st.caption("åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥ä¸Šè¿°é—®é¢˜æ¥è·å–å›ç­”")
        
        st.markdown("---")
        
        # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
        st.subheader("æ¨¡å‹é€‰æ‹©")
        OLLAMA_MODELS = ["qwen3:1.7b"]  # ä»¥åå¯æ‰©å±•ï¼Œå¦‚ ["qwen3:1.7b", "mistral", "gemma:2b"]
        selected_model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            options=OLLAMA_MODELS,
            index=OLLAMA_MODELS.index(st.session_state.get("selected_model", "qwen3:1.7b"))
        )

        # å¦‚æœæ¨¡å‹é€‰æ‹©æ”¹å˜ï¼Œé‡æ–°åˆå§‹åŒ–Agent
        if selected_model != st.session_state.get("selected_model"):
            st.session_state.selected_model = selected_model
            try:
                st.session_state.agent = init_agent(selected_model)
                st.success(f"å·²åˆ‡æ¢åˆ° {selected_model} æ¨¡å‹")
                st.rerun()  # è§¦å‘é‡ç»˜ï¼Œé¿å…æ—§ä¼šè¯ä¸æ–°æ¨¡å‹æ··ç”¨
            except Exception as e:
                st.error(str(e))
        
        st.markdown("---")
        if st.button("æ¸…ç©ºå¯¹è¯å†å²"):
            st.session_state.messages = []
            st.rerun()
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    st.subheader("å¯¹è¯åŒºåŸŸ")
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# è¾“å…¥æ¡†å¤„ç†
def render_chat_input():
    """æ¸²æŸ“èŠå¤©è¾“å…¥æ¡†"""
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        # å…ˆæ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
        col1, col2 = st.columns([3, 1])
        
        # åˆ›å»ºAIå›å¤çš„å ä½ç¬¦
        with col1, st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            # åˆ›å»ºå·¥å…·è°ƒç”¨æ˜¾ç¤ºåŒºåŸŸ
            with col2:
                tool_placeholder = st.empty()
                tool_calls = []
            
            # å¼‚æ­¥è·å–æµå¼å“åº”
            async def get_streaming_response():
                full_response = ""
                try:
                    async for event in st.session_state.agent.stream_async(prompt):
                        # å¤„ç†æ–‡æœ¬äº‹ä»¶
                        if "data" in event:
                            full_response += event["data"]
                            message_placeholder.markdown(full_response + "â–Œ")
                        
                        # å¤„ç†å·¥å…·è°ƒç”¨äº‹ä»¶
                        if "current_tool_use" in event:
                            tool_info = event["current_tool_use"]
                            tool_name = tool_info.get("name", "æœªçŸ¥å·¥å…·")
                            tool_input = tool_info.get("input", {})
                            
                            # è®°å½•å·¥å…·è°ƒç”¨
                            if tool_name not in [t["name"] for t in tool_calls]:
                                tool_calls.append({
                                    "name": tool_name,
                                    "input": tool_input
                                })
                            
                            # æ›´æ–°å·¥å…·è°ƒç”¨æ˜¾ç¤º
                            tool_html = "<div style='background-color:#f0f2f6;padding:10px;border-radius:5px;'>"
                            tool_html += "<h4>å·¥å…·è°ƒç”¨:</h4>"
                            for i, tool in enumerate(tool_calls):
                                tool_html += f"<p><b>{i+1}. {tool['name']}</b></p>"
                            tool_html += "</div>"
                            tool_placeholder.markdown(tool_html, unsafe_allow_html=True)
                    
                    # ç§»é™¤å…‰æ ‡
                    message_placeholder.markdown(full_response)
                    return full_response
                except Exception as e:
                    error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}"
                    message_placeholder.markdown(error_msg)
                    return error_msg
            
            # è¿è¡Œå¼‚æ­¥å‡½æ•°å¹¶è·å–ç»“æœ
            response = asyncio.run(get_streaming_response())
            
            # æ·»åŠ AIå›å¤åˆ°å†å²
            st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()
    render_chat_input()